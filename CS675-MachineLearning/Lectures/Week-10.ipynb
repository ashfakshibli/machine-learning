{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class 1:\n",
    "\n",
    "### Intro to ANN:\n",
    "- XOR (if same false, if different true) - Linear separable items.\n",
    "- FLOPS = Floating point operations per second. (multiplication, addition etc)\n",
    "- Deep Neural Networks\n",
    "- Pytorch by Meta AI, Tensorflow by Google\n",
    "### Review of Classification & Regression:\n",
    "- Response (results - b vector)  vs predictor (features) variables\n",
    "- Floating point numbers - Regreassion\n",
    "- If response binary - Classification -  Logistic Regresssion (Sigmoid Function, Maximize log likelihood, Fit geometrically in sigmoid curve, tweak probability 0/1)\n",
    "    - During training we optimize (beta) values such that values fit the curve. Find optimized curve. Using gradient descent. Minimize log logistic (negated log likelihood)\n",
    "    - Beta1 when becomes negative curve flips.\n",
    "    - if not data approximable by Sigmoid, not reasonable linear separation\n",
    "    - Data - Affine - Activation - Loss fun - Gradient - Stochastic gradient descent\n",
    "### Neurons\n",
    "- Single neuron network very similar to perceptron. Difference is activation function.\n",
    "- No more probability in affine. \n",
    "- Combining neurons and layers make complex functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
