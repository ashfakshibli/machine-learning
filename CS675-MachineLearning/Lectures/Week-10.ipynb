{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class 1:\n",
    "\n",
    "### Intro to ANN:\n",
    "- XOR (if same false, if different true) - Linear separable items.\n",
    "- FLOPS = Floating point operations per second. (multiplication, addition etc)\n",
    "- Deep Neural Networks\n",
    "- Pytorch by Meta AI, Tensorflow by Google\n",
    "### Review of Classification & Regression:\n",
    "- Response (results - b vector)  vs predictor (features) variables\n",
    "- Floating point numbers - Regreassion\n",
    "- If response binary - Classification -  Logistic Regresssion (Sigmoid Function, Maximize log likelihood, Fit geometrically in sigmoid curve, tweak probability 0/1)\n",
    "    - During training we optimize (beta) values such that values fit the curve. Find optimized curve. Using gradient descent. Minimize log logistic (negated log likelihood)\n",
    "    - Beta1 when becomes negative curve flips.\n",
    "    - if not data approximable by Sigmoid, not reasonable linear separation\n",
    "    - Data - Affine (linear + bias -> result to sigmoid) - Activation (Takes input to output) - Loss fun - Gradient - Stochastic gradient descent\n",
    "### Neurons\n",
    "- Single neuron network very similar to perceptron. Difference is activation function.\n",
    "- No more probability in affine. \n",
    "- Combining neurons and layers make complex functions.\n",
    "- Difference is activation function (sigmoid (decision  & probability) vs step)\n",
    "- Naive Bayes (generates new new data from probablity distribution)\n",
    "- In neural network -> causality in context from text data (meaning of data - builds probablity distribution -  What does GPT)\n",
    "- \n",
    "### Single Neuron Network ('Perceptron')\n",
    "- For Logistic regression -> X -> affine (dot prodct, wTx) -> activation (sigmoid function - give probability) -> loss function (maximize likelihood, fit best on given data)\n",
    "- ANN - X -> Affine XBeta-> Activation (p/ y cap) -> Loss function L(beta/W)\n",
    "- Change activation function to step function (single Neuron Network same as Perceptron)\n",
    "- Example using heart data. \n",
    "- 3 neurons network (1 in one sigmoid, 1 in another, another 1 after adding previous 2 to another sigmoid).\n",
    "- Add layers of neurons.\n",
    "- fog = g(f(x)) -> g'(f(x)).f-1(x) (Derivative in ouuter function multiply inner function derivative)\n",
    "- Plug into gradient descent. \n",
    "\n",
    "### NN in action\n",
    "- input  -> hidden -> output (more hidden layer may need less neuron than 1 layer and less prone to overfit)\n",
    "- rectified neural network (RELU)\n",
    "- if want to overfit (1 hidden layer with infinit amouunt of neurons)\n",
    "-  more hidden layer will make the curve more generalized\n",
    "- If the activity is ____ its regression.\n",
    "\n",
    "### Universal approximation theorem\n",
    "- \n",
    "\n",
    "\n",
    "### Outline\n",
    "- Anatomy of NN\n",
    "    - Input X- Hidden W- Output layer Y\n",
    "    - Do not start with Zero. it will make back propagation no update\n",
    "    - Do random initialization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
