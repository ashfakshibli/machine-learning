{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Class 2 (Compuutational Graph)\n",
    "##  The neural networks\n",
    "\n",
    "- Neural network is a combination of functions, can be run parallely\n",
    "- Matrix/Tensors view on NN\n",
    "    - NN is a concatenation of affine functions and activation functions\n",
    "    - Rank in tensor is just dimensionaly\n",
    "    - tensor algebra allows us to scale \n",
    "- Learning as loss minimization. \n",
    "    - min(loss(NN(x, w) or f', y))\n",
    "    - wTx\n",
    "    - L = 1/2 (y - yi)^2\n",
    "    - output of loss functin will be linear combination of sigmoid. \n",
    "- Stochastic gradient descent\n",
    "    - epoch - 1 iteration of on all data points\n",
    "        - shuffle\n",
    "        - minibatch\n",
    "        - gradient w.r.t Ws\n",
    "        - update w <- w - yt * delta L\n",
    "\n",
    "### Chain rule and partial derivatives\n",
    "- The backward pass: Computes derivatives of each intermediate node.\n",
    "- calculate partial derivatives/ gradient\n",
    "- sum of the partial derivatives\n",
    "- Loss should be differentiable\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
