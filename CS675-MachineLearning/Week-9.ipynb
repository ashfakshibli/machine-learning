{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 1:\n",
    "\n",
    "#### SVM\n",
    "\n",
    "- The higher margin of a separation line gives better classification\n",
    "- Linear classifier - the W transpose X = 0 (when the points are on the line)\n",
    "- Hinge Loss (What is it and what does it mean? Exam Quuestion) - Similar to loss in perceptron\n",
    "- Minimizing Regularization and Empirical loss maximize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class 2:\n",
    "\n",
    "### Optimization, SVM and Stochastic Gradient Descent\n",
    "\n",
    "- Convex function\n",
    "- Tangent crosses only one function at one point, have a global minimum.\n",
    "- Objective function is  always 1 dimensional output. Loss. That's youu want to minmize.\n",
    "- Quuadratic functions have shape of Bowl.\n",
    "- Example 1D (Exam Question) - Derivative\n",
    "- Steepest \n",
    "- Solving optimization problem - quaratic programming.\n",
    "- 7 Steps of optimization\n",
    "\n",
    "\n",
    "##### Stochastic Gradient Descent\n",
    " - Mini batch\n",
    " - Hinge Loss - Non Differential function\n",
    " - Detaour: Sub Gradients\n",
    " - Sub gradient  of SVM Objective = if <- is the main point\n",
    " - Epoch - After iteration you have seen all the data.\n",
    " - For epoch if (condition): else: <- main slide\n",
    " - Convergence and learning rate\n",
    " - Perceptron vs SVM\n",
    "\n",
    "##### SVM Duals, Kernels\n",
    "- Kernel methods. Fi (Function) - Explicitly introduuce non-linearity into feature space. Opposite to dimensionality reduction.\n",
    "- Choose W - Margins - Introduce new alpha - linear combination  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
